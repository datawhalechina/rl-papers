
* [Deep Reinforcement Learning from Human Preferences](http://arxiv.org/abs/1706.03741)
> 最早将人类反馈信号引入深度强化学习的文章之一
* [Fine-Tuning Language Models from Human Preferences](http://arxiv.org/abs/1909.08593)
> OpenAI 将 RLHF 运用于 NLP 任务的初步尝试，**代码开源**
* [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)
> OpenAI 采用 RLHF 技术增强语言模型做文章总结的能力
* [WebGPT: Browser-assisted question-answering with human feedback](http://arxiv.org/abs/2112.09332)
> OpenAI 团队解锁了 GPT-3 的网络搜索能力，以更加精准地回答用户问题。可能是 New Bing 的技术积累
* [Teaching language models to support answers with verified quotes](https://arxiv.org/abs/2203.11147)
> DeepMind 团队针对语言模型可能产生令人信服的无稽之谈的问题，通过 RLHF 鼓励语言模型回答问题时引用具体证据
* [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2204.05862)
> Anthropic 团队尝试用 RLHF 技术 fine-tuning 语言模型，提升语言模型的有用性和无害性，并有效地均衡两者之间可能的冲突
* [InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
> OpenAI 采用 RLHF 技术在 NLP 领域的进一步尝试，也是 ChatGPT 的前身
* [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
> 针对 Human feedback 效率低的问题，提出 AI feedback 方案
* [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760v1)
> 研究在大语言模型的背景下，奖励模型导致的 overoptimization 问题，并对于如何训练奖励模型提出了一些建议
* [Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241)
> 【实践】究竟如何将 LM 的任务建模为 MDP？提出了 NLPO 算法，同时评估了强化学习在自然语言处理任务上的性能表现。做了一个 benchmark，代码开源。
* [Tuning computer vision models with task rewards](http://arxiv.org/abs/2302.08242)
> Google 将 RLHF 用于 computer vision 的多项任务
